## (week 4) Decision Trees
- Decision Trees
    - Decision tree model
        - ![Alt text](image.png)
        - ![Alt text](image-1.png)
    - Learning process
        - ![Alt text](image-2.png)
        - Decision 1: How to choose what feature to split on each node
            - Maximize purity (entropy)
        - Decision 2: When do you stop spliiting
            - ![Alt text](image-3.png)
- Decision Tree Learning
    - Measuring purity
        - Entropy: measure of purity of data
        - ![Alt text](image-4.png)
        - ![Alt text](image-5.png)
    - Choosing a split: information gain
        - ![Alt text](image-6.png)
        - ![Alt text](image-7.png)
    - Putting it together
        - ![Alt text](image-8.png)
        - ![Alt text](image-9.png)
    - Using one hot encoding of categorical features
        - ![Alt text](image-10.png)
    - Continous valued features
        - ![Alt text](image-11.png)
- Tree Ensembles
    - Multiple decision trees
        - ![Alt text](image-12.png)
    - Sampling with replacement
        - Technique using statistics to choose different types of decision trees
        - ![Alt text](image-13.png)
        - ![Alt text](image-14.png)
    - Random forest algorithm
        - ![Alt text](image-15.png)
        - ![Alt text](image-16.png)
    - XGBoost
        - Most commonly used algorithm today
        - ![Alt text](image-17.png)
        - ![Alt text](image-18.png)
        - Code implementation:
            - ![Alt text](image-19.png)
    - When to use decision trees
        - ![Alt text](image-20.png)